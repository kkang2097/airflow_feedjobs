{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "6cda29d4",
   "metadata": {},
   "source": [
    "# General Blog Scraping\n",
    "\n",
    "#### Goal:\n",
    "- Scrape any blog that is allowed (after screening robots.txt)\n",
    "\n",
    "#### Outline:\n",
    "- Boilerplate\n",
    "- Scraping a single blog\n",
    "- Trying to scrape multiple different blog formats"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "927f7716",
   "metadata": {},
   "source": [
    "## Boilerplate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "6db9249c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests as req\n",
    "from urllib.parse import urlsplit\n",
    "import urllib.robotparser as urp\n",
    "import re\n",
    "from bs4 import BeautifulSoup\n",
    "from html2text import html2text as htt\n",
    "import sys\n",
    "\n",
    "#Parse RSS\n",
    "def can_scrape(url: str):\n",
    "    #Get robot URL\n",
    "    url_parts = urlsplit(url)\n",
    "    base_url = url_parts.scheme + \"://\" + url_parts.netloc\n",
    "    robot_url = base_url + '/robots.txt'\n",
    "    rp = urp.RobotFileParser()\n",
    "    rp.set_url(robot_url)\n",
    "    rp.read()\n",
    "    return rp.can_fetch(\"*\", url)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "a4e6002e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "True\n",
      "Amazon Polly is a service that turns text into lifelike speech. It enables the\n",
      "development of a whole class of applications that can convert text into speech\n",
      "in multiple languages. This service can be used by chatbots, audio books, and\n",
      "other text-to-speech applications in conjunction with other AWS AI or machine\n",
      "learning (ML) services. For […]\n",
      "\n",
      "\n",
      "Highlight text as it’s being spoken using Amazon Polly\n",
      "Wed, 05 Jul 2023 20:12:48 +0000\n",
      "https://aws.amazon.com/blogs/machine-learning/highlight-text-as-its-being-spoken-using-amazon-polly/\n",
      "248\n"
     ]
    }
   ],
   "source": [
    "url = 'https://aws.amazon.com/blogs/machine-learning/feed/'\n",
    "    #Check robots file\n",
    "    # print(check_robots('https://techinasia.com'))\n",
    "if(can_scrape(url)):\n",
    "    header = {'User-Agent': 'Mozilla/5.0 (Macintosh; Intel Mac OS X 10_10_1) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/39.0.2171.95 Safari/537.36'}\n",
    "    res = req.get('https://aws.amazon.com/blogs/machine-learning/feed/', headers=header)\n",
    "    soup = BeautifulSoup(res.content, features='xml')\n",
    "    articles = soup.findAll('item')\n",
    "\n",
    "    article_list = []\n",
    "    for a in articles:\n",
    "        title = a.find('title').text\n",
    "        link = a.find('link').text\n",
    "        published = a.find('pubDate').text\n",
    "        description = a.find('description').text\n",
    "\n",
    "        article = {\n",
    "            'title': title,\n",
    "            'link': link,\n",
    "            'published': published,\n",
    "            'description': htt(description)\n",
    "            }\n",
    "        article_list.append(article)\n",
    "\n",
    "    idx = 4\n",
    "    print(htt(article_list[idx]['description']))\n",
    "    print(article_list[idx]['title'])\n",
    "    print(article_list[idx]['published'])\n",
    "    print(article_list[idx]['link'])\n",
    "    print(sys.getsizeof(article_list))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "8399cdd7",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "True\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Try this blog post\n",
    "can_scrape(\"https://blog.mithrilsecurity.io\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "0d637bed",
   "metadata": {},
   "source": [
    "### Trying Article Object instead (for various blog formats)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "2a888c71",
   "metadata": {},
   "source": [
    "Okay, article object works wayyy better."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "id": "ea0727e7",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "True\n",
      "When watching a movie or an episode of a TV show, we experience a cohesive narrative that unfolds before us, often without giving much thought to the underlying structure that makes it all possible. However, movies and episodes are not atomic units, but rather composed of smaller elements such as frames, shots, scenes, sequences, and acts. Understanding these elements and how they relate to each other is crucial for tasks such as video summarization and highlights detection, content-based video retrieval, dubbing quality assessment, and video editing. At Netflix, such workflows are performed hundreds of times a day by many teams around the world, so investing in algorithmically-assisted tooling around content understanding can reap outsized rewards.\n",
      "While segmentation of more granular units like frames and shot boundaries is either trivial or can primarily rely on pixel-based information, higher order segmentation¹ requires a more nuanced understanding of the content, such as the narrative or emotional arcs. Furthermore, some cues can be better inferred from modalities other than the video, e.g. the screenplay or the audio and dialogue track. Scene boundary detection, in particular, is the task of identifying the transitions between scenes, where a scene is defined as a continuous sequence of shots that take place in the same time and location (often with a relatively static set of characters) and share a common action or theme.\n",
      "In this blog post, we present two complementary approaches to scene boundary detection in audiovisual content. The first method, which can be seen as a form of weak supervision, leverages auxiliary data in the form of a screenplay by aligning screenplay text with timed text (closed captions, audio descriptions) and assigning timestamps to the screenplay’s scene headers (a.k.a. sluglines). In the second approach, we show that a relatively simple, supervised sequential model (bidirectional LSTM or GRU) that uses rich, pretrained shot-level embeddings can outperform the current state-of-the-art baselines on our internal benchmarks.\n",
      "Screenplays are the blueprints of a movie or show. They are formatted in a specific way, with each scene beginning with a scene header, indicating attributes such as the location and time of day. This consistent formatting makes it possible to parse screenplays into a structured format. At the same time, a) changes made on the fly (directorial or actor discretion) or b) in post production and editing are rarely reflected in the screenplay, i.e. it isn’t rewritten to reflect the changes.\n",
      "In order to leverage this noisily aligned data source, we need to align time-stamped text (e.g. closed captions and audio descriptions) with screenplay text (dialogue and action² lines), bearing in mind a) the on-the-fly changes that might result in semantically similar but not identical line pairs and b) the possible post-shoot changes that are more significant (reordering, removing, or inserting entire scenes). To address the first challenge, we use pre trained sentence-level embeddings, e.g. from an embedding model optimized for paraphrase identification, to represent text in both sources. For the second challenge, we use dynamic time warping (DTW), a method for measuring the similarity between two sequences that may vary in time or speed. While DTW assumes a monotonicity condition on the alignments³ which is frequently violated in practice, it is robust enough to recover from local misalignments and the vast majority of salient events (like scene boundaries) are well-aligned.\n",
      "As a result of DTW, the scene headers have timestamps that can indicate possible scene boundaries in the video. The alignments can also be used to e.g., augment audiovisual ML models with screenplay information like scene-level embeddings, or transfer labels assigned to audiovisual content to train screenplay prediction models.\n",
      "The alignment method above is a great way to get up and running with the scene change task since it combines easy-to-use pretrained embeddings with a well-known dynamic programming technique. However, it presupposes the availability of high-quality screenplays. A complementary approach (which in fact, can use the above alignments as a feature) that we present next is to train a sequence model on annotated scene change data. Certain workflows in Netflix capture this information, and that is our primary data source; publicly-released datasets are also available.\n",
      "From an architectural perspective, the model is relatively simple — a bidirectional GRU (biGRU) that ingests shot representations at each step and predicts if a shot is at the end of a scene.⁴ The richness in the model comes from these pretrained, multimodal shot embeddings, a preferable design choice in our setting given the difficulty in obtaining labeled scene change data and the relatively larger scale at which we can pretrain various embedding models for shots.\n",
      "For video embeddings, we leverage an in-house model pretrained on aligned video clips paired with text (the aforementioned “timestamped text”). For audio embeddings, we first perform source separation to try and separate foreground (speech) from background (music, sound effects, noise), embed each separated waveform separately using wav2vec2, and then concatenate the results. Both early and late-stage fusion approaches are explored; in the former (Figure 4a), the audio and video embeddings are concatenated and fed into a single biGRU, and in the latter (Figure 4b) each input modality is encoded with its own biGRU, after which the hidden states are concatenated prior to the output layer.\n",
      "We have presented two complementary approaches to scene boundary detection that leverage a variety of available modalities — screenplay, audio, and video. Logically, the next steps are to a) combine these approaches and use screenplay features in a unified model and b) generalize the outputs across multiple shot-level inference tasks, e.g. shot type classification and memorable moments identification, as we hypothesize that this path would be useful for training general purpose video understanding models of longer-form content. Longer-form content also contains more complex narrative structure, and we envision this work as the first in a series of projects that aim to better integrate narrative understanding in our multimodal machine learning models.\n"
     ]
    }
   ],
   "source": [
    "if(can_scrape(url)):\n",
    "    #Get HTML\n",
    "    header = {'User-Agent': 'Mozilla/5.0 (Macintosh; Intel Mac OS X 10_10_1) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/39.0.2171.95 Safari/537.36'}\n",
    "    res = req.get('https://netflixtechblog.com/detecting-scene-changes-in-audiovisual-content-77a61d3eaad6', headers=header)\n",
    "#     res = req.get('https://aws.amazon.com/blogs/aws/new-solution-clickstream-analytics-on-aws-for-mobile-and-web-applications/', headers=header)\n",
    "    soup = BeautifulSoup(res.content, features='html')\n",
    "    try:\n",
    "        article = soup.find('article')\n",
    "    except TypeError:\n",
    "        pass\n",
    "    title = article.find('h1').text\n",
    "    body = article.findAll('p')\n",
    "    for idx, p in enumerate(body):\n",
    "        #Smaller paragraphs are usually acknowledgements, etc.\n",
    "        if(sys.getsizeof(p.text) > 120):\n",
    "            print(p.text)\n",
    "#         print(sys.getsizeof(p.text))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
